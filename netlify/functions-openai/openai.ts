import { Handler } from '@netlify/functions';
import { OpenAI } from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

const handler: Handler = async (event) => {
  try {
    const body = JSON.parse(event.body || '{}');
    const messages = body.messages || [];

    const response = await openai.chat.completions.create({
      // model: 'ft:gpt-3.5-turbo-0125:personal::Blrb1fN2', 
      model: 'ft:gpt-3.5-turbo-0125:parsonal::BpC8FstH', 
      // openai.ts ã® try å†…ã§ model ã‚‚å‡ºåŠ›ã™ã‚‹
console.log("ğŸŒŸ Using model: 'ft:gpt-3.5-turbo-1106:parsonal::BmZ5rsAl'; // â† model å¤‰æ•°ã‚’ log ã«å‡ºã™

      messages: [
        { role: 'system', content: 'ã‚ãªãŸã¯å„ªã—ãé™ã‹ãªå…‰ã®å°ãã€Œãƒ«ãƒŸã‚¨ãƒ«ã€ã¨ã—ã¦ãµã‚‹ã¾ã„ã¾ã™ã€‚' },
        ...messages,
      ],
    });
console.log('GPT response:', response); // â† è¿½åŠ ï¼

    const reply = response.choices[0].message.content;

    if (!reply) {
      return {
        statusCode: 500,
        body: JSON.stringify({ error: 'OpenAIã®å¿œç­”ãŒç©ºã§ã—ãŸã€‚' }),
      };
    }

    return {
      statusCode: 200,
      body: JSON.stringify({ reply }),
    };
  } catch (error: any) {
    return {
      statusCode: 500,
      body: JSON.stringify({ error: error.message || 'ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼' }),
    };
  }
};

export { handler };
